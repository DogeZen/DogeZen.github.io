---
title: 深度学习项目nnunet详解
date: 2021-09-21 15:37:51
tags:
- 深度学习
---



# 预处理

#### 重采样

数据集中存在不同spacing的数据，默认自动归一化到数据集所有数据spacing的中值spacing。原始数据使用三阶spline插值；Mask使用最邻近插值。

-  对于各向同性的数据，所有训练集CT的体素尺寸的中位数作为默认值。

  然后利用三阶样条插值（输入数据）和线性插值（标签）进行重采样。

-  对于各向异性的数据，平面以外的轴（z）的目标间隔应当比这个轴上的中位数要小，这样就会生成尽量高分辨率的图像，可以减少重采样的伪影。

  为了实现这样的操作，我们把所有该轴上的spacing的值从小到大排列，取在第10%位置的那个数，作为最终的目标间隔。

  z轴的重采样无论是对输入数据还是对标签（one-hot），都采用最临近插值算法插值。

- UNet Cascade采用特殊的Resample策略：中值尺寸大于显存限制下可处理尺寸的4倍时（batch-size=2），采用级联策略，对数据进行下采样（采样2的倍数，直到满足前面的要求）；如果数据分辨率三个轴方向不相等，先降采样高分辨率轴使得三轴相等，再三轴同时降采样直到满足上述要求。

#### 归一化

- CT：通过统计整个数据集中mask内像素的HU值范围，clip出[0.05,99.5]百分比范围的HU值范围，然后使用z-score方法进行归一化；

- MR：对每个患者数据单独执行z-score归一化。如果crop导致数据集的平均尺寸减小到1/4甚至更小，则只在mask内执行标准化，mask设置为0.

  zscore 归一化 : 先减均值，再除以标准差。

# 数据增强

#### 常规数据增强

(按照以下顺序)

1. **Rotation and Scaling** 

   旋转和scaling都是0.2概率。

   **各向同性的3D** slice，旋转角度αx,αy,αz，都在(-30,30)区间内取任意值。

   **各向异性的3d** 或 **各向同性的2D**，则(-180,180)。

   **各向异性的2d**slice，则取(-15,15)。

    [医学影像中的各向异性(anisotropic)是指什么？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/382284292/answer/1116266847)

   scaling 乘(0.7,1.4)区间的采样。

2. **Gaussian Noise**

   概率0.15 方差(0,0.1)

3. **Gaussian Blur**

   每个样本0.2概率高斯模糊。如果触发了高斯模糊，则该样本的每个模态0.5概率高斯模糊。 

   高斯核取(0.5,1.5)

4. **Brightness**

   概率 0.15 范围(0.7,1.3)

5. **Contrast**

   概率 0.15 范围(0.65,1.5)

6. **Simulation of low resolution**

   每个样本0.2概率触发。如果触发，则该样本的每个模态0.5概率。

   用最近邻插值法(1,2)倍降采样，然后用立方插值法换原回原图大小。

   如果是3d 各向异性和2d ，仅在2D层面数据增强，且用于其原始状态下的平面轴。

7. **Gamma augmentation**

   这种增强是以0.15的概率进行的。

   斑块强度被缩放为其各自数值范围的[0,1]的系数。 

   然后，对每个体素进行γ在(0.7,1.5)范围内的非线性强度转换：
   $$
   i_{new}={i_{old}}^\gamma
   $$
   体素强度随后被缩减到其原始值范围。 

   有0.15概率，体素强度在转换前被倒置：
   $$
   (1-i_{new})=（1-i_{old}）^\gamma
   $$

8. **Mirroring**

   x,y,z轴都0.5概率



#### 特殊数据增强

对于UNet_cascade中的full_resolution_UNet，额外对low_resolution_3D_UNet生成的onehot编码mask采用下面的增强方法。

1. **Binary Operators**

   对所有的labels以0.4的概率进行这个二值操作。

   这个操作是从膨胀、腐蚀、开操作、闭操作中随机选取的。

   结构元素是一个半径为r ~ U(1,8)的球体。(The structure element is a sphere with radiusr∼U(1,8). 这句没看懂)

   该操作以随机顺序应用于标签。因此，onehot编码特性被保留。例如，一个标签的膨胀会导致膨胀区域的所有其他标签被移除。

2. **Removal of Connected Components**

   小于15%patch大小的连通分支,以0.2的概率从onehot编码中移除

# 网络结构

1.ReLU换 leaky ReLU；

2.Batch Norm换Instance Norm

3.网络参数：根据输入图像尺寸不同，在网络结构,patch-size和batch-size进行自适应修改。(留坑，回头细看)
